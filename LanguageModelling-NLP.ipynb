{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#LANGUAGE MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Prediksi Kemunculan Kata Menggunakan Model Bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disusun Oleh:\n",
    "Nama: Vina Fadriani Effendi\n",
    "\n",
    "NIM   : 1301154560"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <i>Natural Language Processing</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Natural Language Processing</i> (NLP) merupakan salah satu cabang ilmu AI yang berfokus pada pengolahan bahasa natural. Saat ini banyak sekali penelitian di bidang pemrosesan bahasa natural. Tetapi, disini penulis akan menjelaskan secara singkat mengenai dasar NLP yaitu <i>Language Modelling</i>. Kita dapat membentuk suatu relasi antar kata dalam suatu kalimat dengan menggunakan metode <i>Language Modelling</i>. Nilai yang menjadi relasi antarkata adalah nilai probabilitas dari antarkata tersebut. Kira-kira bagaimanakah cara menghitung nilai probabilitas tersebut?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cara mengestimasi probabilitas antarkata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Banyak sekali cara untuk menghitung probabilitas antarkata ini. Tapi, apakah kita bisa menghitung nilai tersebut hanya dengan membagi nya seperti ini?\n",
    "<center> ![](files/pict/countprob.png) </center>\n",
    "<b>Tentu saja tidak!</b> Kenapa?\n",
    "\n",
    "Karena jika kita memiliki jumlah kata mencapai 100 kata, maka akan sangat banyak kalimat yang mungkin terbentuk dan kita tidak akan pernah memiliki cukup data untuk melatih program tersebut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i>Markov Assumption</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karena cara menghitung probabilitas biasa tidak akan cukup, maka kita akan menggunakan <i>Markov Assumption</i>.\n",
    "<center> ![](files/pict/markov.png) </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program Prediksi Kata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk membuat sebuah program prediksi kata, kali ini saya telah menyiapkan tepatnya 101 artikel berita tentang teknologi dan sains dari website media online yaitu:\n",
    "* [Teknologi.id](https://www.teknologi.id)\n",
    "* [Kumparan](https://www.kumparan.com)\n",
    "* [Kompas](https://www.tekno.kompas.com)\n",
    "Total kata pada 101 artikel tersebut kurang lebih 37.000 kata.\n",
    "\n",
    "Penulis memilih topik mengenai teknologi karena topik tersebut sangat hangat dikalangan muda dan seringnya tidak mengandung kata-kata yang subjektif. \n",
    "\n",
    "Data artikel yang saya dapat ada pada file 'Dataset-NLP-LanguageModelling - Sheet1', file tersebut berformat CSV dan memiliki 3 kolom. Kolom pertama merupakan link artikel, kolom kedua adalah judul artikel, sedangkan kolom 3 adalah isi artikel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalasi Library\n",
    "\n",
    "Sebelum penulis menjelaskan lebih lanjut, penulis akan meng-import lib yang akan digunakan pada program ini.\n",
    "Library utama adalah:\n",
    "1. CSV untuk membaca file\n",
    "2. NLTK untuk tokenisasi artikel\n",
    "3. MATH untuk menggunakan fungsi matematika dalam perhitungan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fungsi membaca file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi membaca file\n",
    "def openarticle(doc_name):\n",
    "    with open(doc_name, encoding=\"utf-8\") as csvfile: #membuka file csv\n",
    "        rawArticles = csv.reader(csvfile, delimiter=',') #memisah kolom pada file\n",
    "        articles = [] #membuat sebuah array bernama articles untuk menampung semua isi file\n",
    "        for row in rawArticles:\n",
    "            articles.append(row[2].lower()) #menambahkan file ke array articles\n",
    "    return articles #mengembalikan array articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisasi\n",
    "\n",
    "Tokenisasi merupakan cara untuk mengubah data kata menjadi token-token yang biasanya merupakan satu kata ataupun tanda baca yang dipisahkan oleh spasi(' ')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fungsi tokenisasi articles\n",
    "def tokenize(articles):\n",
    "    tokenization = [] #array tokenization untuk menampung hasil tokenisasi\n",
    "    for i in articles:\n",
    "        tokenization.append(nltk.word_tokenize(i)) #menambahkan token ke array tokenization\n",
    "    return tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Unigram\n",
    "\n",
    "Unigram adalah model dari kumpulan kata yang membagi probabilitas kepada satu kata saja. Model ini biasanya akan mengeluarkan kata yang paling sering muncul dari data yang telah kita latih. Sehingga kesinambungan antar kata akan sering tidak jelas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fungsi membangun model unigram\n",
    "def model_unigram(tokenization):\n",
    "    freq_unigram = {} #membuat sebuah dictionary freq_unigram\n",
    "    counter_unigram = 0 #counter_unigram untuk menghitung jumlah kata\n",
    "    for tokens in tokenization: #melooping array tokenization\n",
    "        for token in tokens: #looping pada setiap index tokenization\n",
    "            if token in freq_unigram: #pengecekan apakah kata tersebut sudah ada di dict freq_unigram atau belum\n",
    "                freq_unigram[token] += 1 #jika sudah, maka nilai dict freq_unigram index token akan ditambah 1\n",
    "            else:\n",
    "                freq_unigram[token] = 1 #jika belum, maka nilai dict freq_unigram index token akan menjadi 1\n",
    "            counter_unigram += 1\n",
    "#     print(\"Jumlah kata: \",counter_unigram)\n",
    "    prob_unigram = {} #membuat sebuah dictionary prob_unigram\n",
    "    for token in freq_unigram: \n",
    "        prob_unigram[token] = freq_unigram[token]/counter_unigram #memasukkan nilai dict freq_unigram index token dibagi dengan jumlah kata\n",
    "    prob_unigram\n",
    "    return freq_unigram, prob_unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Bigram\n",
    "\n",
    "Bigram adalah model dari kumpulan kata yang membagi probabilitas kepada dua kata. Model ini akan membangun probabilitas dari 2 kata yang berhubungan/bertetangga dan menghitung probabilitasnya. Sehingga jika kita uji suatu kata, misalnya 'makan' maka, kemungkinan setelah kata makan misalnya 'nasi', 'telur', dan 'masakan'. Program akan mengeluarkan kata yang memiliki nilai probabilitas paling tinggi dengan kata makan. Misalnya yang paling sering muncul adalah 'makan nasi' maka program akan mengeluarkan kata 'nasi'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi membangun model bigram\n",
    "def model_bigram(tokenization):\n",
    "    freq_bigram = {} #membuat sebuah dictionary freq_bigram\n",
    "    counter_bigram = 0 #counter_bigram untuk menghitung jumlah kata\n",
    "    for tokens in tokenization:#melooping array tokenization\n",
    "        for i in range(len(tokens)-1): #looping pada setiap index tokenization\n",
    "            if (tokens[i],tokens[i+1]) in freq_bigram: #pengecekan apakah 2 kata tersebut sudah ada di index dict freq_bigram atau belum\n",
    "                freq_bigram[(tokens[i],tokens[i+1])] += 1 #jika sudah, maka nilai dict freq_bigram index token akan ditambah 1\n",
    "            else:\n",
    "                freq_bigram[(tokens[i],tokens[i+1])] = 1 #jika belum, maka nilai dict freq_bigram index token akan menjadi 1\n",
    "            counter_bigram += 1\n",
    "#     print(\"jumlah kata: \",counter_bigram)\n",
    "    prob_bigram = {} #membuat sebuah dictionary prob_bigram\n",
    "    for token in freq_bigram:\n",
    "        prob_bigram[token] = freq_bigram[token]/counter_bigram #memasukkan nilai dict freq_bigram index token dibagi dengan jumlah pasangan kata\n",
    "    return freq_bigram, prob_bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menampilkan Prediksi Kata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fungsi prediksi kata dari model bigram\n",
    "def suggest(word, prob_bigram):\n",
    "    sggst = {} #membuat sebuah array sggst untuk menampung semua hasil mungkin\n",
    "    max = 0\n",
    "    for key,value in prob_bigram.items(): #looping pada dict prob_bigram\n",
    "        if key[0]==word: #jika key pertama pada setiap index key prob_bigram sama dengan kata yang diinput maka\n",
    "            sggst[key]=value #masukkan key dan nilai pada dict prob_bigram ke dict sggst\n",
    "    print('Hasil kemungkinan: ')\n",
    "    for k,v in sggst.items(): \n",
    "        print(k[1],v) #menampilkan isi dict sggst\n",
    "        if max<v: #pengecekan apakah nilai max probability lebih kecil dari value\n",
    "            max = v #jika ya, maka simpan k dan v ke sebuah variabel max dan temp\n",
    "            temp = k    \n",
    "    return sggst,temp,max #membalikkan array sggst dan kemungkinan kata dengan nilai probabilitas paling besar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fungsi membaca inputan\n",
    "def split_word(sentence):\n",
    "    words = [] #array words untuk menampung hasil split inputan\n",
    "    #sentence = input('Masukkan 1 Kalimat: ')\n",
    "    sentence = sentence.lower() #menginput lalu mengubah kata/kalimat menjadi lowercase\n",
    "    for i in range(len(sentence.split())-1):\n",
    "        words.append((sentence.split()[i],sentence.split()[i+1])) #memasukkan kata yang telah di split ke array words\n",
    "    return words,sentence.split()[len(sentence.split())-1] #mengembalikan array words dan kata terakhir pada inputan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fungsi load data dan membangun model\n",
    "def main_load_data():\n",
    "    articles = openarticle('Dataset-NLP-LanguageModelling - Sheet1.csv') #load data\n",
    "    tokenization = tokenize(articles) #mentokenisasi data\n",
    "    freq_bigram, prob_bigram = model_bigram(tokenization) #membangun model bigram\n",
    "    return prob_bigram #mengembalikan dict prob_bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tes Prediksi Kata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada prediksi kata, penulis akan memilih kata-kata:\n",
    "* artificial\n",
    "* kecerdasan\n",
    "* jagad \n",
    "* cloud\n",
    "* green\n",
    "* rumah\n",
    "* amerika\n",
    "* kuala\n",
    "* media\n",
    "* tempat\n",
    "\n",
    "Alasan yang mendasari penulis dalam pemilihan kata-kata tersebut adalah penulis berfikir bahwa 10 kata diatas memiliki sambungan kata yang sangan jelas dan erat kaitan ataupun besar probabilitasnya. Contohnya artificial memiliki hubungan erat dengan kata intelligence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probability = main_load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kata Artificial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil kemungkinan: \n",
      "intelligence 0.0002900002636366033\n",
      "intelligence/ai 2.63636603306003e-05\n",
      "inteligence 2.63636603306003e-05\n",
      "intellegence 2.63636603306003e-05\n",
      "intelligence/ 2.63636603306003e-05\n",
      "Hasil yang paling mungkin:  intelligence\n",
      "Hasil besar kemungkinan:  0.0002900002636366033\n"
     ]
    }
   ],
   "source": [
    "words,last_word = split_word('artificial')\n",
    "sggst, possible_word, value_possible_word = suggest(last_word,probability)\n",
    "print('Hasil yang paling mungkin: ',possible_word[1])\n",
    "print('Hasil besar kemungkinan: ',value_possible_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kata Kecerdasan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil kemungkinan: \n",
      "buatan 0.0006327278479344072\n",
      "yang 2.63636603306003e-05\n",
      "manusia. 2.63636603306003e-05\n",
      "shuri 2.63636603306003e-05\n",
      "entitas 2.63636603306003e-05\n",
      "diciptakan 2.63636603306003e-05\n",
      "Hasil yang paling mungkin:  buatan\n",
      "Hasil besar kemungkinan:  0.0006327278479344072\n"
     ]
    }
   ],
   "source": [
    "words,last_word = split_word('Kecerdasan')\n",
    "sggst, possible_word, value_possible_word = suggest(last_word,probability)\n",
    "print('Hasil yang paling mungkin: ',possible_word[1])\n",
    "print('Hasil besar kemungkinan: ',value_possible_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kata Jagad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil kemungkinan: \n",
      "raya 2.63636603306003e-05\n",
      "Hasil yang paling mungkin:  raya\n",
      "Hasil besar kemungkinan:  2.63636603306003e-05\n"
     ]
    }
   ],
   "source": [
    "words,last_word = split_word('Jagad')\n",
    "sggst, possible_word, value_possible_word = suggest(last_word,probability)\n",
    "print('Hasil yang paling mungkin: ',possible_word[1])\n",
    "print('Hasil besar kemungkinan: ',value_possible_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kata Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil kemungkinan: \n",
      "computing 2.63636603306003e-05\n",
      "ini 2.63636603306003e-05\n",
      "daripada 2.63636603306003e-05\n",
      ", 5.27273206612006e-05\n",
      "â€ 2.63636603306003e-05\n",
      "yang 2.63636603306003e-05\n",
      ". 7.90909809918009e-05\n",
      "ada 2.63636603306003e-05\n",
      "base 2.63636603306003e-05\n",
      "sudah 2.63636603306003e-05\n",
      "( 2.63636603306003e-05\n",
      ") 2.63636603306003e-05\n",
      "microsoft 2.63636603306003e-05\n",
      "beserta 2.63636603306003e-05\n",
      "Hasil yang paling mungkin:  .\n",
      "Hasil besar kemungkinan:  7.90909809918009e-05\n"
     ]
    }
   ],
   "source": [
    "words,last_word = split_word('Cloud')\n",
    "sggst, possible_word, value_possible_word = suggest(last_word,probability)\n",
    "print('Hasil yang paling mungkin: ',possible_word[1])\n",
    "print('Hasil besar kemungkinan: ',value_possible_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kata Green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil kemungkinan: \n",
      "bank 0.0001845456223142021\n",
      "Hasil yang paling mungkin:  bank\n",
      "Hasil besar kemungkinan:  0.0001845456223142021\n"
     ]
    }
   ],
   "source": [
    "words,last_word = split_word('Green')\n",
    "sggst, possible_word, value_possible_word = suggest(last_word,probability)\n",
    "print('Hasil yang paling mungkin: ',possible_word[1])\n",
    "print('Hasil besar kemungkinan: ',value_possible_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kata Rumah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil kemungkinan: \n",
      ", 2.63636603306003e-05\n",
      "tangga 0.0001054546413224012\n",
      "menggunakan 2.63636603306003e-05\n",
      "sakit 2.63636603306003e-05\n",
      "jangan 2.63636603306003e-05\n",
      "proyektor 2.63636603306003e-05\n",
      "Hasil yang paling mungkin:  tangga\n",
      "Hasil besar kemungkinan:  0.0001054546413224012\n"
     ]
    }
   ],
   "source": [
    "words,last_word = split_word('Rumah')\n",
    "sggst, possible_word, value_possible_word = suggest(last_word,probability)\n",
    "print('Hasil yang paling mungkin: ',possible_word[1])\n",
    "print('Hasil besar kemungkinan: ',value_possible_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kata Amerika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil kemungkinan: \n",
      "serikat 0.0006063641876038069\n",
      "utara 2.63636603306003e-05\n",
      "reggie 2.63636603306003e-05\n",
      "Hasil yang paling mungkin:  serikat\n",
      "Hasil besar kemungkinan:  0.0006063641876038069\n"
     ]
    }
   ],
   "source": [
    "words,last_word = split_word('Amerika')\n",
    "sggst, possible_word, value_possible_word = suggest(last_word,probability)\n",
    "print('Hasil yang paling mungkin: ',possible_word[1])\n",
    "print('Hasil besar kemungkinan: ',value_possible_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kata Kuala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil kemungkinan: \n",
      "lumpur 5.27273206612006e-05\n",
      "Hasil yang paling mungkin:  lumpur\n",
      "Hasil besar kemungkinan:  5.27273206612006e-05\n"
     ]
    }
   ],
   "source": [
    "words,last_word = split_word('Kuala')\n",
    "sggst, possible_word, value_possible_word = suggest(last_word,probability)\n",
    "print('Hasil yang paling mungkin: ',possible_word[1])\n",
    "print('Hasil besar kemungkinan: ',value_possible_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kata Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil kemungkinan: \n",
      "sosial 0.0004218185652896048\n",
      "yang 5.27273206612006e-05\n",
      "luminescent 2.63636603306003e-05\n",
      "penyimpan 2.63636603306003e-05\n",
      ". 2.63636603306003e-05\n",
      "eksrim 2.63636603306003e-05\n",
      "telekomunikasi 2.63636603306003e-05\n",
      "Hasil yang paling mungkin:  sosial\n",
      "Hasil besar kemungkinan:  0.0004218185652896048\n"
     ]
    }
   ],
   "source": [
    "words,last_word = split_word('Media')\n",
    "sggst, possible_word, value_possible_word = suggest(last_word,probability)\n",
    "print('Hasil yang paling mungkin: ',possible_word[1])\n",
    "print('Hasil besar kemungkinan: ',value_possible_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kata Tempat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil kemungkinan: \n",
      "paling 2.63636603306003e-05\n",
      "duduknya 2.63636603306003e-05\n",
      "tujuan 2.63636603306003e-05\n",
      "yang 0.0001318183016530015\n",
      "untuk 0.0001054546413224012\n",
      "itu 5.27273206612006e-05\n",
      "black 2.63636603306003e-05\n",
      ". 2.63636603306003e-05\n",
      "dengan 2.63636603306003e-05\n",
      "lain 2.63636603306003e-05\n",
      "tertinggi 2.63636603306003e-05\n",
      "ini 2.63636603306003e-05\n",
      "tumor 2.63636603306003e-05\n",
      "persembunyiannya 5.27273206612006e-05\n",
      "Hasil yang paling mungkin:  yang\n",
      "Hasil besar kemungkinan:  0.0001318183016530015\n"
     ]
    }
   ],
   "source": [
    "words,last_word = split_word('Tempat')\n",
    "sggst, possible_word, value_possible_word = suggest(last_word,probability)\n",
    "print('Hasil yang paling mungkin: ',possible_word[1])\n",
    "print('Hasil besar kemungkinan: ',value_possible_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari 10 kata yang telah di test, hasil yang diharapkan telah sesuai. Hanya saja pada inputan 'Cloud' program mengembalikan '.'. Artinya, program ini masih mengalami kurang maksimal karena dimana kita tau '.' adalah sebuah tanda baca bukan kata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity atau loss value merupakan nasil loss dari sebuah kalimat menurut penyusunan kata dan besar nilai relasi(probabilitas) antar kata. Nilai ini berbanding terbalik dengan nilai probabilitas sebuah kalimat. Artinya, semakin kecil nilai perplexity maka akan semakin besar probabilitas kalimat tersebut. Adapun rumus mencari perplexity pada model bigram adalah,\n",
    "<center> ![](files/pict/perplex.png) </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fungsi perplexity\n",
    "def getPerplexityValue(sentence, prob_bigram):\n",
    "    perplexity = 0 #inisiasi variabel perplexity dan coounter\n",
    "    counter = 0\n",
    "    for word in range(len(sentence)): #looping untuk setiap pasangan kata pada kalimat\n",
    "        #print(sentence[word])\n",
    "        if (sentence[word]) in prob_bigram: \n",
    "            #print(prob_bigram[(sentence[word])])\n",
    "            perplexity += 1/prob_bigram[(sentence[word])] #menjumlahkan semua hasil 1/nilai probabilitas pasangan kata\n",
    "        counter += 1\n",
    "    return pow(perplexity,1/counter) #mencari nilai akar dari nilai total perplexity terhadap jumlah kata pada kalimat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hitung Preplexity Kalimat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk melakukan pengujian terhadap perhitungan preplexity maka penulis memilih 5 kalimat acak, yaitu:\n",
    "1. Aku ingin makan sayur\n",
    "2. Kecerdasan batin merupakan salah satu teknologi paling diminati.\n",
    "3. Saya ingin menjadi seorang penemu dari aplikasi mobile dan akan membaginya pada akun media sosial saya.\n",
    "4. Kanker merupakan sebuah penyakit yang tidak pernah berkembang\n",
    "5. Ia hanya seorang yang menemukan media anti sosial yang baik.\n",
    "\n",
    "5 kalimat ini ada yang menurut penulis sangat besar kesinambungannya tetapi ada juga yang antarkata tidak berhubungan. Maka dari itu, hasil pengujian akan kita lihat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kalimat 'Aku ingin makan sayur'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil Preplexity:  0.0\n"
     ]
    }
   ],
   "source": [
    "words,last_word = split_word('Aku ingin makan sayur')\n",
    "preplex = getPerplexityValue(words, probability)\n",
    "print('Hasil Preplexity: ',preplex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kalimat 'Kecerdasan batin merupakan salah satu teknologi paling diminati'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil Preplexity:  3.750133361745789\n"
     ]
    }
   ],
   "source": [
    "words,last_word = split_word('Kecerdasan batin merupakan salah satu teknologi paling diminati')\n",
    "preplex = getPerplexityValue(words, probability)\n",
    "print('Hasil Preplexity: ',preplex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kalimat 'Saya ingin menjadi seorang penemu dari aplikasi mobile dan akan membaginya pada akun media sosial saya'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil Preplexity:  2.10383875874134\n"
     ]
    }
   ],
   "source": [
    "words,last_word = split_word('Saya ingin menjadi seorang penemu dari aplikasi mobile dan akan membaginya pada akun media sosial saya')\n",
    "preplex = getPerplexityValue(words, probability)\n",
    "print('Hasil Preplexity: ',preplex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kalimat 'Kanker merupakan sebuah penyakit yang tidak pernah berkembang'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil Preplexity:  4.932147748442422\n"
     ]
    }
   ],
   "source": [
    "words,last_word = split_word('Kanker merupakan sebuah penyakit yang tidak pernah berkembang')\n",
    "preplex = getPerplexityValue(words, probability)\n",
    "print('Hasil Preplexity: ',preplex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kalimat 'Ia hanya seorang yang menemukan media anti sosial yang baik'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil Preplexity:  3.3316488821235812\n"
     ]
    }
   ],
   "source": [
    "words,last_word = split_word('Ia hanya seorang yang menemukan media anti sosial yang baik')\n",
    "preplex = getPerplexityValue(words, probability)\n",
    "print('Hasil Preplexity: ',preplex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis Hasil Preplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasil preplexity sangan bergantung dengan probabilitas hasil antar kata. Hasil dari kalimat yang telah dicoba cenderung kecil, tetapi pada kalimat pertama yaitu \"Aku ingin makan sayur\" hasil preplexity adalah 0. Ini artinya bukanlah kalimat tersebut sangat baik tetapi karena tidak ada kata hasil latih yang berhubungan dengan kata pada kalimat tersebut. Oleh sebab itu, hasil preplexity nya akan sama dengan 0. Untuk menghindari kejadian seperti ini maka, perlu adanya smoothing pada data latih dan data uji. Smoothing pada Language Modelling umumnya menambahkan nilai semua pasangan kata sebanyak x agar pasangan kata yang memiliki jumlah kemuculan 0 tetap akan memiliki nilai. Sehingga ketika data hasil latih digunakan pada percobaan data uji tidak ada satupun bernilai 0 apalagi semua data 0 bernilai preplexity sebesar 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daftar Pustaka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. http://www.cs.brandeis.edu/~cs136a/CS136a_Slides/CS136a_Lect11_PerplexityAndSmoothing.pdf?crazycache=1\n",
    "2. https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
