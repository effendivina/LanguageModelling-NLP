{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['saya pergi ke pasar', 'saya pergi ke sekolah', 'saya makan nasi', 'dia mau belajar', 'dia mau makan nasi']\n"
     ]
    }
   ],
   "source": [
    "with open('artikeltest.csv', encoding=\"utf-8\") as csvfile:\n",
    "    rawArticles = csv.reader(csvfile, delimiter=',')\n",
    "    articles = []\n",
    "    for row in rawArticles:\n",
    "        articles.append(row[2].lower())\n",
    "\n",
    "print(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['saya', 'pergi', 'ke', 'pasar'],\n",
       " ['saya', 'pergi', 'ke', 'sekolah'],\n",
       " ['saya', 'makan', 'nasi'],\n",
       " ['dia', 'mau', 'belajar'],\n",
       " ['dia', 'mau', 'makan', 'nasi']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenization = []\n",
    "for i in articles:\n",
    "    tokenization.append(nltk.word_tokenize(i))\n",
    "tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumlah kata:  18\n",
      "{'saya': 3, 'pergi': 2, 'ke': 2, 'pasar': 1, 'sekolah': 1, 'makan': 2, 'nasi': 2, 'dia': 2, 'mau': 2, 'belajar': 1}\n"
     ]
    }
   ],
   "source": [
    "freq_unigram = {}\n",
    "counter_unigram = 0\n",
    "for tokens in tokenization:\n",
    "    for token in tokens:\n",
    "        if token in freq_unigram:\n",
    "            freq_unigram[token] += 1\n",
    "        else:\n",
    "            freq_unigram[token] = 1\n",
    "        counter_unigram += 1\n",
    "print(\"jumlah kata: \",counter_unigram)\n",
    "print(freq_unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'saya': 0.16666666666666666,\n",
       " 'pergi': 0.1111111111111111,\n",
       " 'ke': 0.1111111111111111,\n",
       " 'pasar': 0.05555555555555555,\n",
       " 'sekolah': 0.05555555555555555,\n",
       " 'makan': 0.1111111111111111,\n",
       " 'nasi': 0.1111111111111111,\n",
       " 'dia': 0.1111111111111111,\n",
       " 'mau': 0.1111111111111111,\n",
       " 'belajar': 0.05555555555555555}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_unigram = {}\n",
    "for token in freq_unigram:\n",
    "    prob_unigram[token] = freq_unigram[token]/counter_unigram\n",
    "prob_unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumlah kata:  13\n",
      "{('saya', 'pergi'): 2, ('pergi', 'ke'): 2, ('ke', 'pasar'): 1, ('ke', 'sekolah'): 1, ('saya', 'makan'): 1, ('makan', 'nasi'): 2, ('dia', 'mau'): 2, ('mau', 'belajar'): 1, ('mau', 'makan'): 1}\n"
     ]
    }
   ],
   "source": [
    "freq_bigram = {}\n",
    "counter_bigram = 0\n",
    "for tokens in tokenization:\n",
    "    for i in range(len(tokens)-1):\n",
    "        if (tokens[i],tokens[i+1]) in freq_bigram:\n",
    "            freq_bigram[(tokens[i],tokens[i+1])] += 1\n",
    "        else:\n",
    "            freq_bigram[(tokens[i],tokens[i+1])] = 1\n",
    "        counter_bigram += 1\n",
    "print(\"jumlah kata: \",counter_bigram)\n",
    "print(freq_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('saya', 'pergi'): 0.15384615384615385,\n",
       " ('pergi', 'ke'): 0.15384615384615385,\n",
       " ('ke', 'pasar'): 0.07692307692307693,\n",
       " ('ke', 'sekolah'): 0.07692307692307693,\n",
       " ('saya', 'makan'): 0.07692307692307693,\n",
       " ('makan', 'nasi'): 0.15384615384615385,\n",
       " ('dia', 'mau'): 0.15384615384615385,\n",
       " ('mau', 'belajar'): 0.07692307692307693,\n",
       " ('mau', 'makan'): 0.07692307692307693}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_bigram = {}\n",
    "for token in freq_bigram:\n",
    "    prob_bigram[token] = freq_bigram[token]/counter_bigram\n",
    "prob_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest(word):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
